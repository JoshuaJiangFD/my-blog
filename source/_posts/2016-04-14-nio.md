---
title: 基于nio构建高扩展服务
published: true
layout: post
tags:
	- NIO
categories:
	- Java
---
如果你需要使用java实现一个高扩展服务，很快Java的nio包将很快进入你的视野。为了让你的server能最终跑起来，你可能会花大量的时间阅读各种博客和指南来理解NIO Selector中需要解决的线程同步问题，或者踩到其他的坑。这篇文章目标是为你描述如何架构一个面向连接的nio-based server，文章将介绍了一个更优化的线程模型，并分析该server的基本组件。

### 线程架构 Threading Architecture

实现一个多线程的server，第一个最容易想到的线程架构就是thread-per-connection模式。这是java 1.4 之前最传统的模式，因为那时候java还不支持非阻塞式IO(non-blocking I/O)。thread-per-connection模式为每个connection分配一个专一的线程。在一个处理循环里，一个工作线程等待新的数据到达，处理请求，然后返回结果，最后调用socket的read方法， 并一直阻塞直到新的数据到达。

```java
/*
 * server based on thread-per-connection pattern 
 */
public class Server {
  private ExecutorService executors = Executors.newFixedThreadPool(10);
  private boolean isRunning = true;

  public static void main(String... args) throws ... {
    new Server().launch(Integer.parseInt(args[0]));
  } 

  public void launch(int port) throws ... {
    ServerSocket sso = new ServerSocket(port);
    while (isRunning) {
      Socket s = sso.accept();
      executors.execute(new Worker(s));
    }
  }

  private class Worker implements Runnable {
    private LineNumberReader in = null;
    ...

    Worker(Socket s) throws ... {
      in = new LineNumberReader(new InputStreamReader(...));
      out = ...
    }

    public void run() {
      while (isRunning) {
        try {
          // blocking read of a request (line) 
          String request = in.readLine();

          // processing the request
          ...
          String response = ...

          // return the response
          out.write(resonse);
          out.flush();
        } catch (Exception e ) { 
          ... 
        }
      }
      in.close();
      ...
    } 
  }
}
```
在这种模式下，所有的并发连接和所有的并发工作线程总是一一对应的。正因为每个连接都对应服务器端的一个线程，单个已经建立的连接的的响应会很迅速。但是高负载下就需要更多并发的工作线程，这限制了可扩展性。并且如果存在大量的long-living connection,例如HTTP长连接，将会导致大量的并发线程，而每个线程的利用率很低，因为有大量时间浪费在等待客户端新的应用层请求（例如HTTP请求，这些HTTP请求复用了未释放的连接）。同样大量并发的线程也浪费内存空间：例如在Solaris JVM上每个线程默认的栈空间大小为512KB。

如果服务器需要的是能并发处理大量的请求，并能接受对单个请求的响应时间被延长，这就需要一种新的更有效率线程架构，例如thread-on-event方式。这种方式下工作线程独立于connection而只被用来处理特定的事件。例如，如果新的数据到达，一个工作线程会被调度去处理应用服务相关的解码和执行具体的应用层任务(或者只是构建一个任务丢到另一个线程池中运行)。一旦调度任务完成，这个工作线程会返回线程池。这种处理方式需要非阻塞的执行socket的I/O操作，即socket上的read和write可以非阻塞式调用，并且需要一个额外的event system,用来通知事件的发生。总之这种方式去除了线程和connection之间的1：1对应关系。这种event-driven的I/O系统称为**Reactor模式**。

### Reactor 模式 Reactor Pattern

Reactor模式（如图1）， 使得事件的监听(如Socker读就绪或写就绪事件)和该事件的处理相分离。例如如果一个读就绪事件被侦听到后，该事件会有一个事先注册好的处理句柄会被调用，但是这个调用可能会发生在另一个可用的分配好的工作线程上。

![reactor model](/images/reactor.png)

- 为了接入事件架构，连接的Channel需要注册到Selector上，这一步通过调用`SelectorChannel.register()`方法，该方法的Selector参数会反过来注册SelectorChannel到自身内部。

```java
SocketChannel channel = serverChannel.accept();
// configure it as non blocking way, required!
channel.configureBlocking(false);
// register the connection
SelectionKey sk = channel.register(selector, SelectionKey.OP_READ);
```

-  为了检测新的事件，Selector 类提供了`select()`方法，收集已经注册的Channel上所有就绪的事件，该方法阻塞直到出现就绪事件。上面的例子里，select()方法会返回所有I/O就绪的connection数目，所有被选择的connection可以通过`selector.selectedKeys()` 得到SelectionKey的集合。每个SelecttionKey上有当前connection上IO事件的状态和对应的Channel引用。

-  Selector引用被Dispatcher持有，Dispatcher是由一个单独的线程循环调用Selector上的`select()`方法侦听IO事件并负责分发事件到对应的EventHandler中。`Selector.select()`方法阻塞等待新的事件，一旦有事件发生，例如对于读就绪事件或者写就绪事件，`select()`方法返回，这时候可以通过调用`selector.selectKeys()`方法获得就绪的channel和对应的IO事件类型。

```java
//Diapatcher's diapatching logic
...
while (isRunning) {
  // blocking call, to wait for new readiness events
  int eventCount = selector.select(); 

  // get the events
  Iterator<SelectionKey> it = selector.selectedKeys().iterator();
  while (it.hasNext()) {
    SelectionKey key = it.next();
    it.remove();

    // readable event?
    if (key.isValid() && key.isReadable()) {
      eventHandler.onReadableEvent(key.channel());
    }

    // writable event? 
    if (key.isValid() && key.isWritable()) {
      key.interestOps(SelectionKey.OP_READ); // reset to read only
      eventHandler.onWriteableEvent(key.channel());
    }
    ...
  }
  ...
}
```
-  EventHandler对于不同的IO事件有不同的处理逻辑，一般包括解码请求，处理请求，编码返回值。因为处理Event handler的工作线程并没有等待连接建立和就绪的过程，系统的瓶颈理论上就只取决于硬件资源如CPU和内存，或者文件句柄数目。这保证了系统的吞吐量，尽管在单个请求的处理时间上可能没有thread-per-connection的快，并且线程切换和同步有一定的额外开销。
所以对于Event-Driven的方法来说，挑战在于如何最小化线程管理的开销，如同步和切换，使得这部分开销可以忽略不计。

### 组件架构 Component Architecture

大部分的高扩展Java服务都基于**Reactor Pattern**, 但是需要做出一些改进，以便支持服务的连接管理，缓存管理和负载均衡。服务的入口称为**Acceptor**，如下图所示:

![reactor server](/images/server.png)

#### Acceptor

**Acceptor**和服务的端口绑定，负责接收所有的客户端请求，它运行在一个专门的线程上。因为它仅仅负责接收请求，这个过程并不耗时，所以可以使用Blocking IO实现，**Acceptor**通过调用`ServerSockerChannel`的`accept`方法建立新的连接，这是一个阻塞调用，新的连接将被注册到`Dispatcher`上。这样这个连接上的IO事件就可以被监听和处理了。

单个到`Dispatcher`的扩展性有限，所以通常需要维护一个小的`Dispatcher Pool`。原因之一是因为大部分操作系统上的`selector`实现是将`SockerChannel`和文件句柄(File Discriptor)一一映射的。所以不同的操作系统上对于单个`selector`上的可用连接数是有不同限制的。

```java
class Acceptor implements Runnable {
  ...
  void init() {
    ServerSocketChannel serverChannel = ServerSocketChannel.open();
    serverChannel.configureBlocking(true);
    serverChannel.socket().bind(new InetSocketAddress(serverPort));
  }

  public void run() {
    while (isRunning) {
      try {
        SocketChannel channel = serverChannel.accept(); 
        /*
         * a Connection object holds the SocketChannel and an Application-Level event handler
         */
        Connection con = new Connection(channel, appHandler);
        dispatcherPool.nextDispatcher().register(con);  
      } catch (...) {
        ...
      }
    }
  }
}
```

#### Dispatcher

在上面的Acceptor处理逻辑中会调用Dispatcher注册新连接，`Dispacther.Register()`会调用内部的`Selector`来注册该`SocketChannel`。此处有一问题, `Selector`内部通过`key sets` 管理已注册的Channels, 注册新的`channel`时，需要创建一个`SelectionKey`并添加到`key sets`中，而在并发操作下，同一个`Dispatcher`同样会通过`Selector.select()`方法读取`key sets`。因为`key sets`本身不是thread-safe,因此需要实现额外的线程同步。对于该问题有一个范式: `selector guard object idiom`通过暂停`Dispatcher`线程避免了可能的死锁和Race Condition问题。

```java
class Dispatcher implements Runnable {
  private Object guard = new Object();
  ...

  void register(Connection con) {
    // retrieve the guard lock and wake up the dispatcher thread
    // to register the connection's channel
    synchronized (guard) {
      selector.wakeup();  
      con.getChannel().register(selector, SelectionKey.OP_READ, con);
    }

    // notify the application EventHandler about the new connection 
    ...
  }

  void announceWriteNeed(Connection con) {
    SelectionKey key = con.getChannel().keyFor(selector);
    synchronized (guard) {
      selector.wakeup();
      key.interestOps(SelectionKey.OP_READ | SelectionKey.OP_WRITE);
    }
  }

  public void run() {
    while (isRunning) {
      synchronized (guard) {
        // suspend the dispatcher thead if guard is locked 
      }
      int eventCount = selector.select();

      Iterator<SelectionKey> it = selector.selectedKeys().iterator();
      while (it.hasNext()) {
        SelectionKey key = it.next(); 
        it.remove();

        // read event?
        if (key.isValid() && key.isReadable()) {
          Connection con = (Connection) key.attachment();
          disptacherEventHandler.onReadableEvent(con);
        }
        // write event?
        ...
      }
    }
  }
}
```
一旦connection注册完成，Selector会监听connection上的IO事件，事件发生后，Dispatcher上对应的处理函数会调用，同时向处理函数传入对应的connection。

#### Dispatcher-Level EventHandler

当读就绪事件发生时，首先将执行`Channel`上的`read`方法， 和流式的读取方式不同的是，`Channel`上的`read`方法需要传入一块缓冲区，通常情况下这里会使用直接缓冲区(Direct Buffer)，直接缓冲区是直接在进程内存空间上分配的一块区域，绕开了Java的堆空间。使用直接缓冲区，Socket上的IO操作不再需要创建中间缓冲区。
通常情况下，read方法执行时间很快。不同的操作系统具体会有所不同，通常情况下核心态下读操作只是将接收到的数据从kernel spacce拷贝一份到用户进程内存的一块读缓冲中。然后接收到的数据会被追加到连接对应的一个线程安全的读队列中等待后续处理。在Dispatcher完成Channel上的IO操作后，将交由对应的应用层任务处理，这会调用相应的Application-Level EventHander，并一般会交给一个分配的工作线程去执行。

```java
class DispatcherEventHandler {
  ...

  void onReadableEvent(final Connection con) {
    // get the received data 
    ByteBuffer readBuffer = allocateMemory();
    con.getChannel().read(readBuffer);
    ByteBuffer data = extractReadAndRecycleRenaming(readBuffer);

    // append it to read queue
    con.getReadQueue().add(data); 
    ...

    // perform further operations (encode, process, decode) 
    // by a worker thread
    if (con.getReadQueue().getSize() &gt; 0) {
      workerPool.execute(new Runnable() {
        public void run() {
          synchronized (con) {
            con.getAppHandler().onData(con);
          }
        }
      }); 
    }
  }

  void onWriteableEvent(Connection con) {
    ByteBuffer[] data = con.getWriteQueue().drain();
    con.getChannel().write(data); // write the data
    ...

    if (con.getWriteQueue().isEmpty()) {
      if (con.isClosed()) {
        dispatcher.deregister(con);
      }

    } else {
       // there is remaining data to write
       dispatcher.announceWriteNeed(con); 
    }
  }
}
```

#### Application-Level EventHandler

`Dispatcher`主要用来监听和处理SockerChannel上IO事件，而Application-Level EventHandler处理的是已经抽象过的面向连接的事件，如连接建立，连接断开。注意这里的连接应该指代应用层面对的连接如HTTP连接，FTP连接。不同的应用层框架对的EventHandler有不同的定义，如SEDA, MINA, EmberIO等。这些NIO框架通常定义了多层的事件处理链。这样就可以定义例如SSLHandler或者DelayedWriteHandler对请求和结果进行拦截。下面是一个xSocket框架的Application-Level EventHandler示例。`xSocket`框架支持不同的处理接口，这些接口定义的回调方法可以实现不同的应用层逻辑。

```java
class POP3ProtocolHandler implements IConnectHandler, IDataHandler, ... {
  private static final String DELIMITER = ...
  private Mailbox mailbox = ...


  public static void main(String... args) throws ... {
    new MultithreadedServer(110, new POP3ProtocolHandler()).run();
  }

  public boolean onConnect(INonBlockingConnection con) throws ... {
    if (gatekeeper.isSuspiciousAddress(con.getRemoteAddress())) {
      con.setWriteTransferRate(5);  // reduce transfer: 5byte/sec
    }

    con.write("+OK My POP3-Server" + DELIMITER);
    return true;
  }

  public boolean onData(INonBlockingConnection con) throws ... {
    String request = con.readStringByDelimiter(DELIMITER);

    if (request.startsWith("QUIT")) {
      mailbox.close();
      con.write("+OK POP3 server signing off" + DELIMITER);
      con.close();

    } else if (request.startsWith("USER")) {
      this.user = request.substring(4, request.length());
      con.write("+OK enter password" + DELIMITER);


    } else if (request.startsWith("PASS")) {
      String pwd = request.substring(4, request.length());
      boolean isAuthenticated = authenticator.check(user, pwd);
      if (isAuthenticated) {
        mailbox = MailBox.openAndLock(user);
        con.write("+OK mailbox locked and ready" + DELIMITER);
      } else {
        ...
      }  
    } else if (...) {
      ...
    }
    return true;
  }
}
```
Dispatcher会将读取好的数据放在连接对应的Read队列中，为了方便应用层读取队列中的数据，`Connection`对象提供了若干read/write方法支持以stream或者channel的方式操作数据，对写队列也是一样。

通常关闭连接时，NIO框架底层实现会flush掉Connection对象的write对流里面的数据，所有数据写完之后，连接会被关闭。除了正常的连接关闭之外，其他例如硬件错误也会关闭一个TCP-based的连接。这种情况只有在往socket写数据或者读数据时或者idle timeout时才会被发现。大部分基于NIO的框架大多实现了对连接异常断开的处理。

#### 结论

事件驱动的非阻塞架构是实现高效可扩展可依赖服务的基础层。困难在于如何最小化线程同步和切换的开销，并最优化连接和缓存的管理。这些重要的工作已经不需要重复实现，一些服务框架如xSocket, emberIO, SEDA和MINA都对底层的事件处理和线程管理进行了抽象，使得开发人员可以专注在应用层逻辑上。大部分这些框架还支持了UDP和SSL协议。 这些都未在本文展开了。

#### Reference
[原文链接Architecture of a highly scalable NIO-based server](https://community.oracle.com/docs/DOC-983601)
